{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut_boeahimZG",
        "outputId": "d737181a-7b98-4336-8566-6a2701f84bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/RL+MOOC list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    link = row[' link']\n",
        "    try:\n",
        "        # Send a GET request to the web page\n",
        "        response = requests.get(link)\n",
        "        # Create a BeautifulSoup object from the response content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find the HTML element containing the publication information\n",
        "        publication_info = soup.find('span', class_='publication-date')\n",
        "        # Extract the publication year\n",
        "        publication_year = publication_info.get_text().split()[-1]\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('research_papers_with_years.xlsx', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/RL+MOOC list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    link = row[' link']\n",
        "    try:\n",
        "        # Send a GET request to the web page\n",
        "        response = requests.get(link)\n",
        "        # Extract the publication year using regular expressions\n",
        "        publication_year = re.search(r\"\\b\\d{4}\\b\", response.text).group()\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('research_papers_with_years1.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "2Rs8lDMCj_HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import dateparser\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/RL+MOOC list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    link = row[' link']\n",
        "    try:\n",
        "        # Send a GET request to the web page\n",
        "        response = requests.get(link)\n",
        "        # Extract the publication date from the response text\n",
        "        publication_date = dateparser.parse(response.text)\n",
        "        # Extract the year from the publication date\n",
        "        publication_year = publication_date.year\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('research_papers_with_years2.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "nXTwZQGZk0xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scholarly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM8eWGIck_qt",
        "outputId": "0277c761-df9b-4c5d-a617-1bc6bbf65230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.11.2)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.0.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from scholarly)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.1.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx (from scholarly)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from scholarly)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.27.1)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->scholarly) (2.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from bibtexparser->scholarly) (3.1.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scholarly) (1.14.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from free-proxy->scholarly) (4.9.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->scholarly)\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.26.16)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio~=0.17 (from selenium->scholarly)\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sphinx<7,>=1.6 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (3.5.4)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (0.16)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx->scholarly)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->scholarly) (3.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.14.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (23.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Collecting async-generator>=1.9 (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.1.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.1.3)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.0-py3-none-any.whl size=42429 sha256=e696d99f3eff12eeaa6c4f1fea903016d8437e2c415a73238034b04508c07f98\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/de/d5/4e42fd75d48106e7c4023d6594c2992db38afa7019f96139a2\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.1-py3-none-any.whl size=5636 sha256=e2977ffc44d8295f8a870c2c37c80b57273bbd6cbc6f790c8fdd26cacf8501fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/96/c7/5a434714fff4fea9a59075428b142626e0a74f8c3bf90a50d0\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: fake-useragent, python-dotenv, outcome, h11, deprecated, bibtexparser, async-generator, wsproto, trio, httpcore, free-proxy, arrow, trio-websocket, sphinxcontrib-jquery, httpx, sphinx-rtd-theme, selenium, scholarly\n",
            "Successfully installed arrow-1.2.3 async-generator-1.10 bibtexparser-1.4.0 deprecated-1.2.14 fake-useragent-1.1.3 free-proxy-1.1.1 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 outcome-1.2.0 python-dotenv-1.0.0 scholarly-1.7.11 selenium-4.10.0 sphinx-rtd-theme-1.2.2 sphinxcontrib-jquery-4.1 trio-0.22.0 trio-websocket-0.10.3 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/RL+MOOC list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    link = row[' link']\n",
        "    try:\n",
        "        # Send a GET request to the web page\n",
        "        response = requests.get(link)\n",
        "        # Create a BeautifulSoup object from the response content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find the HTML element containing the publication date\n",
        "        publication_date_elem = soup.find('meta', attrs={'name': 'DC.Date'})\n",
        "        # Extract the publication date from the 'content' attribute of the meta tag\n",
        "        publication_date = publication_date_elem['content']\n",
        "        # Extract the publication year from the date\n",
        "        publication_year = publication_date.split('-')[0]\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('research.xlsx', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp2UmUtbl3yk",
        "outputId": "b5ad6666-857f-4350-ab4f-fafd49802dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googlesearch import search\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/RL+MOOC list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    paper_name = row['Name']\n",
        "    try:\n",
        "        # Search the research paper name on Google\n",
        "        search_results = search(paper_name, num_results=1)\n",
        "        # Get the first search result URL\n",
        "        first_result_url = next(iter(search_results))\n",
        "        # Send a GET request to the first search result URL\n",
        "        response = requests.get(first_result_url)\n",
        "        # Create a BeautifulSoup object from the response content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find the HTML element containing the publication date\n",
        "        publication_date_elem = soup.find('span', class_='publish-date')\n",
        "        # Extract the publication date from the element text\n",
        "        publication_date = publication_date_elem.get_text()\n",
        "        # Extract the publication year from the date\n",
        "        publication_year = publication_date.split()[-1]\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('research_papers_with_years.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "puhIZwKmp6I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googlesearch import search\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    paper_name = row['title']\n",
        "    try:\n",
        "        # Search the research paper name on Google\n",
        "        search_results = search(paper_name, num_results=1, lang='en')\n",
        "        # Get the first search result snippet\n",
        "        first_result_snippet = next(iter(search_results))\n",
        "        # Extract the publication year from the snippet\n",
        "        publication_year = None\n",
        "        if '20' in first_result_snippet:\n",
        "            publication_year = first_result_snippet.split('20', 1)[1][:4]\n",
        "        elif '19' in first_result_snippet:\n",
        "            publication_year = first_result_snippet.split('19', 1)[1][:4]\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('re.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "JC4cLd_8q3h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googlesearch import search\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    paper_name = row['title']\n",
        "    try:\n",
        "        # Search the research paper name on Google\n",
        "        search_results = search(paper_name, num_results=1, lang='en')\n",
        "        # Get the first search result snippet\n",
        "        first_result_snippet = next(iter(search_results))\n",
        "        # Extract the publication year from the snippet\n",
        "        publication_year = None\n",
        "        if '20' in first_result_snippet:\n",
        "            publication_year = first_result_snippet.split('20', 1)[1][:4]\n",
        "        elif '19' in first_result_snippet:\n",
        "            publication_year = first_result_snippet.split('19', 1)[1][:4]\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('r.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "Cf098h45rzBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googlesearch import search\n",
        "import re\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('list.xlsx')\n",
        "\n",
        "# Create a new column to store the publication years\n",
        "df['Publication Year'] = None\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    paper_name = row['title']\n",
        "    try:\n",
        "        # Search the research paper name on Google\n",
        "        search_results = search(paper_name, num_results=1, lang='en')\n",
        "        # Get the first search result snippet\n",
        "        first_result_snippet = next(iter(search_results))\n",
        "        # Extract the publication year from the snippet using regular expressions\n",
        "        publication_year = re.search(r\"\\d{4}\", first_result_snippet).group()\n",
        "        # Update the 'Publication Year' column in the DataFrame\n",
        "        df.at[index, 'Publication Year'] = publication_year\n",
        "    except:\n",
        "        # Handle any errors that occur during scraping\n",
        "        df.at[index, 'Publication Year'] = 'Error'\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel('res.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "ECn-TDIGsgbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}